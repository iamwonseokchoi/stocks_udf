{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import datetime\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../..')\n",
    "from data_engineering.shared import read_parquet_from_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import optuna\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model, Model, model_from_json, Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, MultiHeadAttention, Attention, RepeatVector, Conv1D, Flatten, GRU\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import pandas_market_calendars as mcal\n",
    "import gc\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(ticker):\n",
    "    df = read_parquet_from_s3(ticker)\n",
    "    df.set_index('t', inplace=True)\n",
    "    df_embedded = df['embedded_text'].apply(pd.Series)\n",
    "    df_embedded = df_embedded.rename(columns=lambda x: 'embed_' + str(x))\n",
    "    df = pd.concat([df, df_embedded], axis=1)\n",
    "    df.drop('embedded_text', axis=1, inplace=True)\n",
    "    cols = list(df.columns)\n",
    "    cols.remove('target')\n",
    "    cols.append('target')\n",
    "    df = df[cols]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = scaler.fit_transform(df.values)\n",
    "\n",
    "    def create_rolling_sequences_with_timestamps(data, timestamps, seq_length):\n",
    "        X, y, ts = [], [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            seq = data[i: i + seq_length]\n",
    "            X.append(seq[:-1])\n",
    "            y.append(seq[-1, -1])\n",
    "            ts.append(timestamps[i + seq_length - 1])\n",
    "        return np.array(X), np.array(y), np.array(ts)\n",
    "\n",
    "    seq_length = lookback\n",
    "    X, y, timestamps = create_rolling_sequences_with_timestamps(scaled_df, df.index.values, seq_length)\n",
    "\n",
    "    train_size = int(len(scaled_df) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X, y, scaler, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "def combined_metric(y_true, y_pred):\n",
    "    \n",
    "    # Convert inputs to tensors if they're not\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Calculating RMSE\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "    \n",
    "    # Calculating MAE\n",
    "    mae = tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "    \n",
    "    # Calculating R-squared\n",
    "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - SS_res / (SS_tot + tf.keras.backend.epsilon())\n",
    "    \n",
    "    # Calculating Directional Accuracy\n",
    "    direction_true = tf.sign(y_true - tf.reduce_mean(y_true))\n",
    "    direction_pred = tf.sign(y_pred - tf.reduce_mean(y_pred))\n",
    "    directional_accuracy = tf.reduce_mean(tf.cast(tf.equal(direction_true, direction_pred), dtype=tf.float32))\n",
    "    \n",
    "    # Combined metric (normalized by range for RMSE and MAE, increased weight for r2 and directional accuracy)\n",
    "    y_range = tf.reduce_max(y_true) - tf.reduce_min(y_true)\n",
    "    combined = (rmse/y_range + 0.5 * mae/y_range - 1.5 * r2 - directional_accuracy)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, model_type, params):\n",
    "    if model_type == 'lstm':\n",
    "        return create_lstm_model(input_shape, params)\n",
    "    elif model_type == 'gru':\n",
    "        return create_gru_model(input_shape, params)\n",
    "    elif model_type == 'wavenet':\n",
    "        return create_wavenet_model(input_shape, params)\n",
    "    elif model_type == 'simple_transformer':\n",
    "        return create_simple_transformer_model(input_shape, params)\n",
    "    elif model_type == 'stacked':\n",
    "        return create_stacked_model(input_shape, params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# LSTM Model\n",
    "def create_lstm_model(input_shape, params):\n",
    "    optimizer = Adam(clipnorm=1.0)  \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['unit_number'], input_shape=input_shape, return_sequences=True,\n",
    "                    kernel_regularizer=l2(params['l2_strength']), \n",
    "                    recurrent_regularizer=l2(params['l2_strength']), \n",
    "                    bias_regularizer=l2(params['l2_strength'])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(LSTM(params['unit_number'],\n",
    "                    kernel_regularizer=l2(params['l2_strength']), \n",
    "                    recurrent_regularizer=l2(params['l2_strength']), \n",
    "                    bias_regularizer=l2(params['l2_strength'])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[combined_metric])\n",
    "    return model\n",
    "\n",
    "# GRU Model\n",
    "def create_gru_model(input_shape, params):\n",
    "    optimizer = Adam(clipnorm=1.0)  \n",
    "    model = Sequential()\n",
    "    model.add(GRU(params['unit_number'], input_shape=input_shape, return_sequences=True,\n",
    "                    kernel_regularizer=l2(params['l2_strength']), \n",
    "                    recurrent_regularizer=l2(params['l2_strength']), \n",
    "                    bias_regularizer=l2(params['l2_strength'])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(GRU(params['unit_number'], \n",
    "                    kernel_regularizer=l2(params['l2_strength']), \n",
    "                    recurrent_regularizer=l2(params['l2_strength']), \n",
    "                    bias_regularizer=l2(params['l2_strength'])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[combined_metric])\n",
    "    return model\n",
    "\n",
    "# WaveNet Model\n",
    "def create_wavenet_model(input_shape, params):\n",
    "    optimizer = Adam(clipnorm=1.0)\n",
    "    model = Sequential()\n",
    "    for _ in range(params.get('n_layers', 2)):\n",
    "        model.add(Conv1D(params['filter_number'], 2, dilation_rate=2**_, padding='causal', activation=params['activation']))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[combined_metric])\n",
    "    return model\n",
    "\n",
    "# Simple-Transformer Model\n",
    "def create_simple_transformer_model(input_shape, params):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    attention_output = MultiHeadAttention(num_heads=params['num_heads'], key_dim=params['key_dim'])(query=input_tensor, key=input_tensor, value=input_tensor)\n",
    "    x = Dropout(params['dropout_rate'])(attention_output)\n",
    "    x = Flatten()(x)\n",
    "    output_tensor = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    optimizer = Adam(clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[combined_metric])\n",
    "    # model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Stacked Model (Conv1D + LSTM + LSTM)\n",
    "def create_stacked_model(input_shape, params):\n",
    "    optimizer = Adam(clipnorm=1.0)  \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=params['filter_number'], kernel_size=min(4, params['kernel_size']), input_shape=input_shape, activation=params.get('activation', 'relu')))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(params['unit_number'], return_sequences=True,\n",
    "                    kernel_regularizer=l2(params['l2_strength']), \n",
    "                    recurrent_regularizer=l2(params['l2_strength']), \n",
    "                    bias_regularizer=l2(params['l2_strength'])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(LSTM(params['unit_number'], \n",
    "                    kernel_regularizer=l2(params['l2_strength']), \n",
    "                    recurrent_regularizer=l2(params['l2_strength']), \n",
    "                    bias_regularizer=l2(params['l2_strength'])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[combined_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, **kwargs):\n",
    "    X_train = kwargs['X_train']\n",
    "    y_train = kwargs['y_train']\n",
    "    X_test = kwargs['X_test']\n",
    "    y_test = kwargs['y_test']\n",
    "    \n",
    "    model_type = trial.suggest_categorical('model_type', [\n",
    "        'lstm', 'gru', 'wavenet', 'simple_transformer', 'stacked'\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "        'unit_number': trial.suggest_int('unit_number', 20, 180),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0, 0.6),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [5, 10, 20, 40, 50, 60, 80, 100]),\n",
    "        'epochs': trial.suggest_int('epochs', 25, 100),\n",
    "        'filter_number': trial.suggest_int('filter_number', 8, 512),\n",
    "        'kernel_size': trial.suggest_int('kernel_size', 1, 3),\n",
    "        'l2_strength': trial.suggest_float('l2_strength', 1e-6, 1e-1, log=True),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "        'num_heads': trial.suggest_int('num_heads', 1, 16),\n",
    "        'key_dim': trial.suggest_int('key_dim', 1, 16),\n",
    "        'lr_factor': trial.suggest_float('lr_factor', 0.1, 0.9, step=0.1),\n",
    "        'min_lr': trial.suggest_float('min_lr', 1e-5, 1e-2, log=True),\n",
    "    }\n",
    "    input_shape = (X.shape[1], X.shape[2])\n",
    "    \n",
    "    log_dir = f\"{log_base_path}{model_type}-{trial.number}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=params['lr_factor'], patience=patience_lr, min_lr=params['min_lr'])\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        model = create_lstm_model(input_shape, params)\n",
    "    elif model_type == 'gru':\n",
    "        model = create_gru_model(input_shape, params)\n",
    "    elif model_type == 'wavenet':\n",
    "        model = create_wavenet_model(input_shape, params)\n",
    "    elif model_type == 'simple_transformer':\n",
    "        model = create_simple_transformer_model(input_shape, params)\n",
    "    elif model_type == 'stacked':\n",
    "        model = create_stacked_model(input_shape, params)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience_early_stop)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=params['lr_factor'], patience=patience_lr, min_lr=params['min_lr'])\n",
    "\n",
    "    callbacks_list = [\n",
    "        early_stopping, \n",
    "        reduce_lr, \n",
    "        tensorboard_callback, \n",
    "        TFKerasPruningCallback(trial, 'val_loss')\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_test, y_test), \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        verbose=1, \n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "    combined_val_metric = history.history['combined_metric'][-1]\n",
    "    \n",
    "    return combined_val_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(X_train, y_train, X_test, y_test, study):\n",
    "    best_params = study.best_params\n",
    "    best_model_type = best_params['model_type']\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = create_model(input_shape, best_model_type, best_params)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience_early_stop)\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        validation_data=(X_test, y_test), \n",
    "        epochs=best_params['epochs'], \n",
    "        batch_size=best_params['batch_size'], \n",
    "        verbose=1, \n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Best Model is: '{best_model_type}'\")\n",
    "    print(\"-\" * 30)\n",
    "    for key, value in best_params.items():\n",
    "        if key != 'model_type':\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    return model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X, y, scaler, timestamps, ticker, best_model_type, time):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_inv = scaler.inverse_transform(np.concatenate([X[:, 0, :-1], y_pred], axis=1))[:, -1]\n",
    "    y_real_inv = scaler.inverse_transform(np.concatenate([X[:, 0, :-1], y.reshape(-1, 1)], axis=1))[:, -1]\n",
    "    compare_df = pd.DataFrame({'Real': y_real_inv, 'Predicted': y_pred_inv})\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    blue = sns.color_palette(\"deep\")[0]\n",
    "    orange = sns.color_palette(\"deep\")[1]\n",
    "\n",
    "    image_save_path = f\"../images/networks/{ticker}_{best_model_type}-{time}.png\"\n",
    "\n",
    "    sns.lineplot(x=timestamps, y=compare_df['Real'], label='Real', color=blue, linewidth=2.5)\n",
    "    sns.lineplot(x=timestamps, y=compare_df['Predicted'], label='Predicted', color=orange, linewidth=2.5)\n",
    "\n",
    "    plt.xlabel('Time Index (Daily)', fontsize=16)\n",
    "    plt.ylabel('Delta (Close-Open) Price', fontsize=16)\n",
    "    plt.legend(frameon=True, loc='upper right', fontsize='medium')\n",
    "    plt.title(f\"Actual vs. Predictions ['{best_model_type}' Model Architecture: {ticker}-{time}]\", fontsize=20)\n",
    "\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    steps = 20*3\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.lineplot(x=timestamps[-(steps):], y=compare_df[-(steps):]['Real'], label='Real', color=blue, linewidth=2.5)\n",
    "    sns.lineplot(x=timestamps[-(steps):], y=compare_df[-(steps):]['Predicted'], label='Predicted', color=orange, linewidth=2.5)\n",
    "\n",
    "    plt.xlabel('Time Index (Daily)', fontsize=16)\n",
    "    plt.ylabel('Delta (Close-Open) Price', fontsize=16)\n",
    "    plt.legend(frameon=True, loc='upper right', fontsize='medium')\n",
    "    plt.title(f\"Actual vs. Predictions [{best_model_type}: {ticker}-{time}] (Last {steps} Trading Days)\", fontsize=20)\n",
    "\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 5\n",
    "patience_early_stop = 5\n",
    "train_for_seconds = (60 * 60) * 0.5\n",
    "max_trials = 25\n",
    "patience_lr = 5\n",
    "\n",
    "model_save_path = \"../models/networks/\"\n",
    "time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Main loop\n",
    "# for ticker in json.loads(os.getenv('TICKERS')):\n",
    "for ticker in ['TSLA']:\n",
    "    \n",
    "    log_base_path = f\"../logs/tensorboard/{ticker}_\"\n",
    "    X_train, y_train, X_test, y_test, X, y, scaler, timestamps = preprocess_data(ticker)\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    objective_with_data = functools.partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "    study.optimize(objective_with_data, n_trials=max_trials, timeout=train_for_seconds)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Train the best model\n",
    "    best_model, best_params = train_best_model(X_train, y_train, X_test, y_test, study)\n",
    "\n",
    "    # Save the best model\n",
    "    model_json = best_model.to_json()\n",
    "    with open(f\"{model_save_path}{ticker}_{best_params['model_type']}_{time}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    best_model.save_weights(f\"{model_save_path}{ticker}_{best_params['model_type']}_{time}_weights.keras\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    visualize_predictions(best_model, X, y, scaler, timestamps, best_params['model_type'], ticker, time)\n",
    "    \n",
    "    # Cleanup\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".xgb_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
